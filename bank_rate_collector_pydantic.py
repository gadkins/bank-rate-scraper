# -*- coding: utf-8 -*-
"""bank_rate_collector_pydantic.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zASeVTqvnv9k5erWSLnWObHJ641yy2nT

## Install dependencies
"""

"""## Set environment variables"""

# If opening this notebook in Google Colab, add OPENAI_API_KEY to Secrets
import os
from google.colab import userdata
os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')

"""## Define Pydantic models

**Notes:**

- `Field(None)`: Instead of setting fields directly to None, we use `Field(None)` to avoid implicitly setting a `default` key in the JSON schema.
- `Field(default_factory=list)`: For lists, we use `default_factory=list` to ensure that the schema generation does not add an unnecessary `default` key.
"""

from typing import List, Union
from pydantic import BaseModel

class CheckingAccountResponse(BaseModel):
    name: str
    interestRate: float
    annualPercentageYield: Union[float, None]
    minimumBalanceToObtainAPY: Union[float, None]
    minimumBalanceToOpen: Union[float, None]
    minimumDailyBalance: Union[float, None]
    dividendRate: Union[float, None]
    dividendFrequency: Union[str, None]

class SavingsAccountResponse(BaseModel):
    name: str
    interestRate: Union[float, None]
    annualPercentageYield: float
    minimumBalanceToObtainAPY: Union[float, None]
    minimumBalanceToOpen: Union[float, None]
    minimumDailyBalance: Union[float, None]
    dividendRate: Union[float, None]
    dividendFrequency: Union[str, None]

class MoneyMarketAccountResponse(BaseModel):
    name: str
    interestRate: Union[float, None]
    annualPercentageYield: float
    minimumBalanceToObtainAPY: Union[float, None]
    dividendRate: Union[float, None]
    dividendFrequency: Union[str, None]
    minimumBalanceToOpen: Union[float, None]
    minimumDailyBalance: Union[float, None]

class CertificateOfDepositResponse(BaseModel):
    term: str
    interestRate: Union[float, None]
    annualPercentageYield: float
    minimumBalanceToObtainAPY: Union[float, None]
    minimumBalanceToOpen: Union[float, None]
    minimumDailyBalance: Union[float, None]

class IndividualRetirementAccountResponse(BaseModel):
    term: str
    interestRate: Union[float, None]
    annualPercentageYield: float
    minimumBalanceToObtainAPY: Union[float, None]
    minimumBalanceToOpen: Union[float, None]
    minimumDailyBalance: Union[float, None]

class LoanResponse(BaseModel):
    name: str
    term: Union[Union[int, str], None]
    annualPercentageRate: float
    minimumPayment: Union[float, None]
    maximumLoanAmount: Union[float, None]
    paymentPer1000Dollars: Union[float, None]

class CreditCardResponse(BaseModel):
    name: Union[str, None]
    annualPercentageRate: float
    annualFee: Union[float, None]
    doesEarnRewards: Union[bool, None]

class FeeResponse(BaseModel):
    name: str
    feeAmount: float
    feeUnit: str
    oneTime: Union[bool, None]
    recurringInterval: Union[str, None]

class BankResponse(BaseModel):
    bankRootDomain: str
    checkingAccounts: Union[List[CheckingAccountResponse], None]
    savingsAccounts: Union[List[SavingsAccountResponse], None]
    moneyMarketAccounts: Union[List[MoneyMarketAccountResponse], None]
    certificatesOfDeposit: Union[List[CertificateOfDepositResponse], None]
    individualRetirementAccounts: Union[List[IndividualRetirementAccountResponse], None]
    loans: Union[List[LoanResponse], None]
    creditCards: Union[List[CreditCardResponse], None]
    fees: Union[List[FeeResponse], None]

"""## Functions for scraping html tables and converting to CSV"""

import time
import random
import requests
from bs4 import BeautifulSoup
from bs4.element import Tag
from typing import List, Dict, Optional, Tuple
import csv
import io
from openai import OpenAI
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from concurrent.futures import ThreadPoolExecutor, as_completed
import os
from urllib.parse import urlparse


def fetch_tables(url: str, session: requests.Session, headers: Dict[str, str]) -> Tuple[str, Optional[List[Tag]]]:
    try:
        response = session.get(url, headers=headers)
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'html.parser')
            tables = soup.find_all('table')
            return url, tables
        else:
            # Fall back to Selenium if requests fails
            # Useful if websites load content dynamically with JavaScript.
            chrome_options = Options()
            chrome_options.add_argument('--headless')
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            driver = webdriver.Chrome(service=Service(), options=chrome_options)

            driver.get(url)
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, 'table'))
            )
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            tables = soup.find_all('table')
            driver.quit()
            return url, tables
    except requests.RequestException as e:
        print(f"Failed to retrieve {url} with requests: {e}")
        return url, None
    except Exception as e:
        print(f"An error occurred with {url} using Selenium: {e}")
        return url, None

# Given a list of URLs, return a dictionary where each key is a URL and each value
# is a list of Tag objects representing the HTML table(s) for the given URL
def extract_all_tables_from_html(
    urls: List[str],
    max_workers: int = 10
) -> Tuple[Dict[str, Optional[List[Tag]]], List[Tuple[str, str]]]:
    """
    Given a list of URLs, return a dictionary where each key is a URL and each value
    is a list of Tag objects representing the HTML table(s) for the given URL, and
    a list of unsuccessful URLs with error messages.

    Args:
        urls (List[str]): List of URLs to process.
        max_workers (int): Maximum number of worker threads.

    Returns:
        Tuple[Dict[str, Optional[List[Tag]]], List[Tuple[str, str]]]:
        A dictionary with URLs as keys and lists of Tag objects as values, and
        a list of tuples containing unsuccessful URLs and their error messages.
    """
    result: Dict[str, Optional[List[Tag]]] = {}
    unsuccessful_urls: List[Tuple[str, str]] = []

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept-Language': 'en-US,en;q=0.9',
        'Accept-Encoding': 'gzip, deflate, br',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Connection': 'keep-alive',
        'sec-ch-ua': '"Not/A)Brand";v="8", "Chromium";v="126", "Google Chrome";v="126"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-platform': '"macOS"',
        'sec-fetch-dest': 'document',
        'sec-fetch-mode': 'navigate',
        'sec-fetch-site': 'none',
        'sec-fetch-user': '?1',
        'upgrade-insecure-requests': '1'
    }

    session = requests.Session()

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_url = {executor.submit(fetch_tables, url, session, headers): url for url in urls}

        for future in as_completed(future_to_url):
            url = future_to_url[future]
            try:
                url, tables = future.result()
                result[url] = tables
            except Exception as e:
                print(f"An error occurred with {url}: {e}")
                result[url] = None
                unsuccessful_urls.append((url, str(e)))

    return result, unsuccessful_urls

def get_domain_from_url(url: str) -> str:
    parsed_url = urlparse(url)
    return parsed_url.netloc

def table_to_csv(table: Tag, domain: str) -> str:
    output = io.StringIO()
    writer = csv.writer(output)

    first_row = True
    for row in table.find_all('tr'):
        cols = row.find_all(['td', 'th'])
        row_data = [col.get_text(strip=True) for col in cols]
        if first_row:
            row_data.insert(0, domain)
            first_row = False
        writer.writerow(row_data)

    return output.getvalue()

def convert_tables_to_csv(results: Dict[str, Optional[List[Tag]]]) -> Dict[str, List[str]]:
    csv_results: Dict[str, List[str]] = {}

    for url, tables in results.items():
        if tables is None:
            csv_results[url] = []
            continue

        domain = get_domain_from_url(url)
        csv_tables = []
        for table in tables:
            csv_content = table_to_csv(table, domain)
            csv_tables.append(csv_content)

        csv_results[url] = csv_tables

    return csv_results

"""## Define print functions"""

# Print the CSV content for each URL
def print_csv_tables(csv_tables_dict):
    for url, csv_tables in csv_tables_dict.items():
        print(f"CSV Tables from {url}:")
        for i, csv_table in enumerate(csv_tables):
            print(f"Table {i+1}:")
            print(csv_table)
            print()  # Print a newline for better readability

"""## Scrape sites

'https://www.simplicity.coop/rates',
    'https://www.bankofdeerfield.bank/resources/deposit-rates',
    'https://www.salemcoop.com/rates-fees/deposit-interest-rates/',
    'https://www.bankwithpremier.com/Current%20Deposit%20Interest%20Rates',
    'https://www.shrewsburycu.com/home/member-services/rates',
    'https://www.parkbank.bank/Pages/savings-cd.html',
    'https://www.parkbank.bank/Pages/p-checking.html',
    'https://www.parkbank.bank/Pages/rewards-checking.html',
    'https://www.bluffviewbank.com/personal/savings/cd-rates/',
    'https://www.bluffviewbank.com/personal/checking/checking-and-savings-rates/',
    'https://www.dcu.org/bank/certificates/regular-certificates.html',
    'https://www.dcu.org/bank/savings.html',
    'https://www.dcu.org/bank/checking.html',
    'https://www.dcu.org/bank/retirement/ira-savings.html',
    'https://www.firstiowa.bank/personal/cd-and-ira-rates',
    'https://www.firstiowa.bank/connect/rates',
    'https://www.firstiowa.bank/personal/checking',
    'https://www.huntington.com/Personal/savings-cds-overview/certificates-of-deposit',
    'https://www.huntington.com/Personal/savings-cds-overview/relationship-money-market-account',
    'https://www.huntington.com/Personal/checking/perks',
    'https://www.huntington.com/Personal/savings-cds-overview/money-market-ira',
    'https://www.huntington.com/Personal/savings-cds-overview/premier-savings-account',
    'https://verveacu.com/personal/product/savings/certificates/share-certificates/',
    'https://verveacu.com/personal/product/savings/money-market/',
    'https://verveacu.com/personal/product/checking/kickback-checking/',
    'https://verveacu.com/personal/product/savings/kickback-savings/',
    'https://verveacu.com/personal/product/savings/money-market/',
"""

# Example usage
urls = [
    'https://www.simplicity.coop/rates',
    'https://www.bankofdeerfield.bank/resources/deposit-rates',
    'https://www.salemcoop.com/rates-fees/deposit-interest-rates/',
    'https://www.bankwithpremier.com/Current%20Deposit%20Interest%20Rates',
    'https://www.shrewsburycu.com/home/member-services/rates',
    'https://www.parkbank.bank/Pages/savings-cd.html',
    'https://www.parkbank.bank/Pages/p-checking.html',
    'https://www.parkbank.bank/Pages/rewards-checking.html',
    'https://www.bluffviewbank.com/personal/savings/cd-rates/',
    'https://www.bluffviewbank.com/personal/checking/checking-and-savings-rates/',
    'https://www.dcu.org/bank/certificates/regular-certificates.html',
    'https://www.dcu.org/bank/savings.html',
    'https://www.dcu.org/bank/checking.html',
    'https://www.dcu.org/bank/retirement/ira-savings.html',
    'https://www.firstiowa.bank/personal/cd-and-ira-rates',
    'https://www.firstiowa.bank/connect/rates',
    'https://www.firstiowa.bank/personal/checking',
    #'https://www.huntington.com/Personal/savings-cds-overview/certificates-of-deposit',
    #'https://www.huntington.com/Personal/savings-cds-overview/relationship-money-market-account',
    #'https://www.huntington.com/Personal/checking/perks',
    #'https://www.huntington.com/Personal/savings-cds-overview/money-market-ira',
    #'https://www.huntington.com/Personal/savings-cds-overview/premier-savings-account',
    #'https://verveacu.com/personal/product/savings/certificates/share-certificates/',
    #'https://verveacu.com/personal/product/savings/money-market/',
    #'https://verveacu.com/personal/product/checking/kickback-checking/',
    #'https://verveacu.com/personal/product/savings/kickback-savings/',
    #'https://verveacu.com/personal/product/savings/money-market/',
    #'https://www.fandmstbk.com/uploads/userfiles/files/documents/2024_05_20%20Consumer%20Deposit%20Rates.pdf',
]

# Step 1: Extract tables from the list of URLs
results, unsuccessful_urls = extract_all_tables_from_html(urls)
# results, unsuccessful_urls = extract_all_rates_from_pdfs(urls)

# Step 2: Convert the extracted tables to CSV format
csv_tables_dict = convert_tables_to_csv(results)

# Print the unsuccessful URLs and their exceptions
for url, error in unsuccessful_urls:
    print(f"Failed to retrieve {url}: {error}")

# Print successfult csv tables
print_csv_tables(csv_tables_dict)
# print_extracted_rates(results)

"""## Functions to chunk and send request to LLM"""

from pydantic import BaseModel
from typing import List, Tuple

def chunk_data(data: str, chunk_size: int) -> List[str]:
    """
    Splits a large string into smaller chunks of a specified maximum size.

    Args:
        data (str): The data to be chunked.
        chunk_size (int): The maximum size of each chunk.

    Returns:
        List[str]: A list of data chunks.
    """
    chunks = []
    for i in range(0, len(data), chunk_size):
        chunks.append(data[i:i + chunk_size])
    return chunks

def extract_with_llm(chunk: str) -> BankResponse:
    prompt = f"""
Extract the banking rate data from the following text and structure it according to the provided model.

Special instructions:
- If a property or object does not exist, do not include it in the output.
- Do not include 'www' or other subdomains in the bankRootDomain.
- If dividend rate is given, do not include interest rate.
- Do not convert percentage to decimal. I.e. if the rate is 0.55%, return 0.55 not 0.0055

Text:
{chunk}
"""
    client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])

    response = client.beta.chat.completions.parse(
        model="gpt-4o-mini-2024-07-18",
        messages=[
            {"role": "system", "content": "You are a helpful assistant designed to output structured data."},
            {"role": "user", "content": prompt}
        ],
        response_format=BankResponse,  # Directly use the Pydantic model here
    )

    return response.choices[0].message.parsed  # This will be a Pydantic model instance

"""## Funtion to merge BankResponses by URL"""

from collections import defaultdict
from typing import List, Dict, Union
from urllib.parse import urlparse

def merge_bank_responses(responses: List[BankResponse]) -> BankResponse:
    """
    Merges a list of BankResponse objects into a single BankResponse object.

    Args:
        responses (List[BankResponse]): A list of BankResponse objects to merge.

    Returns:
        BankResponse: A merged BankResponse object.
    """
    if not responses:
        return None

    merged_response = BankResponse(
        bankRootDomain=responses[0].bankRootDomain,
        checkingAccounts=[],
        savingsAccounts=[],
        moneyMarketAccounts=[],
        certificatesOfDeposit=[],
        individualRetirementAccounts=[],
        loans=[],
        creditCards=[],
        fees=[]
    )

    for response in responses:
        if response.checkingAccounts:
            merged_response.checkingAccounts.extend(response.checkingAccounts)
        if response.savingsAccounts:
            merged_response.savingsAccounts.extend(response.savingsAccounts)
        if response.moneyMarketAccounts:
            merged_response.moneyMarketAccounts.extend(response.moneyMarketAccounts)
        if response.certificatesOfDeposit:
            merged_response.certificatesOfDeposit.extend(response.certificatesOfDeposit)
        if response.individualRetirementAccounts:
            merged_response.individualRetirementAccounts.extend(response.individualRetirementAccounts)
        if response.loans:
            merged_response.loans.extend(response.loans)
        if response.creditCards:
            merged_response.creditCards.extend(response.creditCards)
        if response.fees:
            merged_response.fees.extend(response.fees)

    return merged_response

"""## Function to merge BankResponses by Root Domain"""

def merge_responses_by_domain(responses: List[BankResponse]) -> Dict[str, BankResponse]:
    """
    Merges BankResponse objects that share the same root domain.

    Args:
        responses (List[BankResponse]): A list of BankResponse objects.

    Returns:
        Dict[str, BankResponse]: A dictionary where the keys are root domains and the values are merged BankResponse objects.
    """
    domain_responses = defaultdict(list)

    for response in responses:
        domain_responses[response.bankRootDomain].append(response)

    merged_responses = {
        domain: merge_bank_responses(responses)
        for domain, responses in domain_responses.items()
    }

    return merged_responses

"""## Function to process and extract the main CSV tables dictionary"""

def process_and_extract_tables(
    csv_tables_dict: Dict[str, List[str]],
    chunk_size: int
) -> Tuple[List[BankResponse], List[str]]:
    """
    Processes the CSV tables and extracts structured data using OpenAI's API,
    returning a list of merged BankResponse objects and a list of unsuccessful URLs.

    Args:
        csv_tables_dict (Dict[str, List[str]]): Dictionary where keys are URLs and values are lists of CSV tables.
        chunk_size (int): The size of the data chunks to be processed.

    Returns:
        Tuple[List[BankResponse], List[str]]: A tuple containing a list of merged BankResponse objects and a list of unsuccessful URLs.
    """
    url_responses: Dict[str, List[BankResponse]] = defaultdict(list)
    unsuccessful_urls: List[str] = []

    for url, csv_tables in csv_tables_dict.items():
        if not csv_tables:
            unsuccessful_urls.append(url)
            continue

        start_time = time.time()
        combined_csv = ''.join(csv_tables)
        chunks = chunk_data(combined_csv, chunk_size)

        for chunk in chunks:
            extracted_data_chunk = extract_with_llm(chunk)
            url_responses[url].append(extracted_data_chunk)

        end_time = time.time()
        processing_time = end_time - start_time

        print(f"Processing time for {url}: {processing_time:.2f} seconds")

    # Merge responses by URL
    merged_url_responses = [
        merge_bank_responses(responses) for responses in url_responses.values()
    ]

    # Merge responses by root domain
    final_merged_responses = list(merge_responses_by_domain(merged_url_responses).values())

    return final_merged_responses, unsuccessful_urls

"""## Function to print Pydantic object as JSON"""

import json
from typing import List

def print_bank_responses_json(bank_responses: List[BankResponse], num_to_print: int) -> None:
    """
    Prints the specified number of BankResponse objects in JSON format.

    Args:
        bank_responses (List[BankResponse]): A list of BankResponse objects to print.
        num_to_print (int): The number of BankResponse objects to print.
    """
    for i, bank_response in enumerate(bank_responses[:num_to_print]):
        print(json.dumps(bank_response.dict(), indent=4))
        print("\n" + "-" * 40 + "\n")

def normalize_domain(domain: str) -> str:
    """
    Normalizes the domain by removing the 'www.' prefix if it exists.

    Args:
        domain (str): The domain to normalize.

    Returns:
        str: The normalized domain.
    """
    if domain.startswith("www."):
        return domain[4:]
    return domain

def print_bank_response_as_json_by_domain(bank_responses: List[BankResponse], domain: str) -> None:
    """
    Prints the BankResponse object in JSON format for the specified bankRootDomain.

    Args:
        bank_responses (List[BankResponse]): A list of BankResponse objects to search through.
        domain (str): The bankRootDomain or www subdomain of the BankResponse object to print.
    """
    normalized_domain = normalize_domain(domain)

    for bank_response in bank_responses:
        normalized_response_domain = normalize_domain(bank_response.bankRootDomain)

        if normalized_response_domain == normalized_domain:
            print(json.dumps(bank_response.dict(), indent=4))
            print("\n" + "-" * 40 + "\n")
            return

    print(f"No BankResponse found for domain: {domain}")

"""# Run extraction process"""

# Step 3: Process the CSV tables and extract structured data using OpenAI's API
extracted_data, unsuccessful_urls = process_and_extract_tables(csv_tables_dict, chunk_size=500)

# Print the unsuccessful URLs
for url in unsuccessful_urls:
    print(f"Failed to retrieve {url}")

# print_bank_responses_json(extracted_data, num_to_print=2)
# print_bank_response_as_json_by_domain(extracted_data, "huntington.com")
print_bank_responses_json(extracted_data, num_to_print=5)